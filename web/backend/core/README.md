# Backend Core Documentation

This document provides a detailed breakdown of the files in the `web/backend/core/` directory.

## `config.py`

### Purpose

This file is responsible for managing the application's configuration. It centralizes settings for the environment, LLM, application behavior, and logging.

### Key Functionality

1.  **Environment Variable Loading**: It uses `python-dotenv` to load secrets and settings from a `.env` file located within the `core` directory.
2.  **LLM Configuration**: Configures the Gemini model (`LLM_MODEL`) and retrieves the `GOOGLE_API_KEY`. It performs a critical check to ensure the API key is provided, raising a `ValueError` if it's missing.
3.  **Application Settings**: Defines key operational parameters:
    *   `UPLOAD_FOLDER`: The destination for file uploads.
    *   `MAX_CONTENT_LENGTH`: The maximum size for uploaded files (defaulting to 16MB).
    *   `ALLOWED_EXTENSIONS`: A set of permitted file extensions (`.xlsx`, `.xls`).
4.  **CORS Configuration**: Manages Cross-Origin Resource Sharing by defining a list of `ALLOWED_ORIGINS` to prevent unauthorized cross-site requests.
5.  **Logging Setup**: The `setup_logging` function provides a robust, application-wide logging system that includes:
    *   Automatic creation of a `logs` directory.
    *   Both file and console handlers with explicit UTF-8 encoding and error replacement to gracefully handle Unicode.
    *   A standardized log format for consistency.
    *   Suppression of overly verbose logs from third-party libraries like `httpx` and `langchain`.
6.  **File Validation**: Provides a helper function `is_allowed_file(filename)` to quickly verify if an uploaded file has a permitted extension.

### Analysis and Recommendations

*   **[Security-High] Secret Management**: The `.env` file containing the `GOOGLE_API_KEY` is located inside the `core` source directory. This is a significant security risk. If the source code is ever leaked, the API key will be compromised.
    *   **Recommendation**: For development, the `.env` file should be moved to the project root and added to `.gitignore`. For production, secrets should be managed through a secure vault (like Google Secret Manager or AWS Secrets Manager) and accessed via environment variables set on the server, not from a file within the codebase.

*   **[Inefficiency] Redundant Logging Function**: The `safe_log_message` function is redundant. The logging system is already configured with `errors='replace'` on the file handler, which prevents crashes from `UnicodeEncodeError`. This function adds unnecessary complexity.
    *   **Recommendation**: Remove the `safe_log_message` function and use direct logger calls (e.g., `logger.info()`, `logger.error()`) throughout the application.

*   **[Clarity] Unused Code**: The file contains commented-out code at the end, which should be removed to improve code clarity and maintainability.

## `extract_df.py`

### Purpose

This module is the bridge between the conceptual query plan from the LLM agents and the concrete data stored in a pandas DataFrame. Its primary role is to take the hierarchically structured row and column selections (generated by the `Row Handler` and `Column Handler` agents) and use them to filter the DataFrame, producing a final, targeted view of the data for the user.

### Key Functionality

1.  **Main Entry Point**: The `render_filtered_dataframe()` function orchestrates the entire filtering process.
2.  **Selection String Parsing**:
    *   `parse_row_paths()`: Ingests the LLM-generated, indentation-based string for row selection and transforms it into a list of structured "paths". Each path dictates a specific set of conditions to select rows.
    *   `parse_col_paths()`: Performs the same parsing logic for the column selection string.
3.  **Pandas Condition Generation**:
    *   `create_row_condition()`: Constructs a sophisticated boolean mask for row filtering. It correctly interprets multiple values for a condition (translating to `isin` checks) and combines different paths with `OR` logic, while conditions within a single path are combined with `AND`.
    *   `create_column_tuples()`: Translates the parsed hierarchical column paths into a definitive list of pandas MultiIndex tuples, ready for direct use in column selection.
4.  **MultiIndex Navigation**:
    *   Includes a helper function `search_column_in_multiindex()` to robustly find the full, correct column tuple even when only a partial name is provided, which is essential for mapping LLM output to the actual DataFrame structure.

### Analysis and Recommendations

*   **[Bug-High] Flawed Row Parsing Logic**: The `parse_row_paths` function's logic for handling hierarchies is oversimplified. It correctly identifies top-level items but fails to correctly process nested structures, potentially overwriting or misinterpreting hierarchical conditions. This could lead to incorrect or incomplete data being returned from a query.
    *   **Recommendation**: The parsing logic should be rewritten to use a stack-based approach. This would allow it to correctly manage and build multiple hierarchical paths simultaneously, respecting the indentation levels to navigate up and down the hierarchy tree.

*   **[Performance] Inefficient Column Search**: The `create_column_tuples` function iterates through every column in the DataFrame for every single path in the user's column selection. For DataFrames with a large number of columns, this nested loop will result in significant performance degradation.
    *   **Recommendation**: For a more scalable solution, pre-process the DataFrame's columns into a more efficient search structure, such as a Trie or a multi-level dictionary, upon file load. This would change the column lookup from a linear scan to a much faster, near-constant time operation.

*   **[Bug-Medium] Type-Unsafe Comparisons**: In `create_row_condition`, filtering values from the LLM's text output are compared directly with DataFrame values. This can lead to silent errors if the DataFrame column has a numeric type (e.g., `int` or `float`) and the LLM provides a string representation (e.g., `"2023"` vs. `2023`).
    *   **Recommendation**: Before applying a filter, attempt to cast the string value from the LLM to the `dtype` of the target DataFrame column. This will ensure type consistency and prevent filtering errors. The operation should be wrapped in a `try...except` block to handle cases where a cast is not possible.

*   **[Maintainability] Brittle Parsing**: The parsing functions rely on counting leading spaces to determine the hierarchy. This is fragile and highly dependent on the LLM's output format remaining perfectly consistent. Any small change in formatting (e.g., tabs instead of spaces, different indentation levels) will break the parser.
    *   **Recommendation**: The most robust solution is to change the LLM prompt to request output in a structured format like JSON. This would eliminate the need for fragile text parsing entirely and make the system more reliable. If this is not feasible, the existing functions should be documented with explicit examples of the expected format.

## `preprocess.py`

### Purpose

This module provides a suite of functions for cleaning and standardizing the raw pandas DataFrame loaded from an Excel file. Preprocessing is a critical initial step to handle inconsistencies and prepare the data for reliable metadata extraction and query processing.

### Key Functionality

1.  **Header-Only Extraction**: The `extract_headers_only()` function offers an efficient method to read just the header rows of an Excel file without loading the entire dataset into memory. This is used to get a quick understanding of the file's column structure.
2.  **Header Name Cleaning**: `clean_unnamed_header()` targets a common pandas behavior where empty header cells are loaded as `"Unnamed: X"`. It renames these to a consistent `"Header"`, which simplifies downstream logic.
3.  **Hierarchical Gap Filling**: `fill_undefined_sequentially()` is a custom function intended to fill in `NaN` values in hierarchical columns with a placeholder (`"Undefined"`) based on a set of specific rules about the surrounding data.
4.  **Forward Filling**: `forward_fill_column_nans()` implements the standard `ffill` (forward-fill) method. This is essential for hierarchical data where parent items are often listed only once and their value needs to be propagated to subsequent rows.

### Analysis and Recommendations

*   **[Bug-Critical] Function Will Crash**: The `fill_undefined_sequentially` function is fundamentally broken. It attempts to slice a 1D boolean array with 2D indexing (`[:,0]`), which will raise an `IndexError` during execution. Furthermore, the implemented logic does not match the logic described in its docstring, which refers to checking the *next row*, a feature that appears to have been commented out.
    *   **Recommendation**: This function must be completely rewritten. First, its intended purpose needs to be clarified. Then, the logic should be implemented correctly without the faulty indexing and in alignment with its documented goal.

*   **[Maintainability] Foreign Language Identifiers**: The `clean_unnamed_header` function contains a nested function named `sua_ten_tieu_de_phu` (Vietnamese for "fix sub-heading name"). Using multiple languages in a codebase is detrimental to long-term maintainability and collaboration.
    *   **Recommendation**: All identifiers (variables, functions) should be translated to English for consistency.

*   **[Inefficiency] Repetitive Renaming**: The `clean_unnamed_header` function calls `df.rename` inside a loop, which is inefficient.
    *   **Recommendation**: The function should first build a single dictionary containing all the name changes and then make a single call to `df.rename` to perform all modifications at once.

*   **[Clarity] Redundant Type Checking**: `forward_fill_column_nans` contains boilerplate code to handle various input types for the column list. This adds verbosity and makes the function's expected input less clear.
    *   **Recommendation**: Refactor the function to expect a single, well-defined type (i.e., `list[str]`). The responsibility for providing the correct type should lie with the calling code. This simplifies the function and clarifies its API contract.

## `metadata.py`

### Purpose

This module is a cornerstone of the chatbot's intelligence. It is responsible for programmatically understanding the structure of an Excel file by converting its implicit row and column hierarchies into explicit, nested dictionary representations. This structured metadata is what allows the LLM to reason about the contents of the file.

### Key Functionality

1.  **Row Header Detection**: `get_number_of_row_header()` uses a heuristic to guess the number of rows that constitute the file's header. It inspects the first column for consecutive `NaN` values, assuming they represent merged vertical header cells.
2.  **Column Hierarchy Conversion**: `convert_df_headers_to_nested_dict()` is a complex recursive function that transforms a pandas MultiIndex (hierarchical columns) into a nested dictionary. It includes logic to prune branches at specific keywords (like "Header") and to flatten the final leaves of the hierarchy into a list for conciseness.
3.  **Row Hierarchy Conversion**: `convert_df_rows_to_nested_dict()` performs a similar recursive conversion for the data within the rows. It takes a list of columns that define the hierarchy and builds a nested dictionary representing the relationships between the data elements.

### Analysis and Recommendations

*   **[Bug-High] Inefficient and Fragile Header Detection**: The `get_number_of_row_header` function loads the *entire* Excel file into memory just to inspect the first few cells of one column. This is extremely inefficient and will fail on large files. The heuristic of counting `NaN`s is also brittle and will fail for many valid Excel layouts.
    *   **Recommendation**: For efficiency, the function should be modified to read only the first N rows (e.g., `nrows=20`) of the Excel file. For better accuracy, the header structure should be determined by an LLM call, which is more robust at interpreting varied layouts.

*   **[Maintainability] Code Clarity and Internationalization**: The code is difficult to maintain due to the mix of English and Vietnamese in comments and log messages. Furthermore, `logging` is imported and instantiated inside functions, which is not standard practice and can lead to unexpected behavior.
    *   **Recommendation**: Translate all comments and log messages to English. Move `import logging` and the `logger` instantiation to the top of the module for consistency and predictability.

*   **[Complexity] Overly Complex Recursive Logic**: The recursive functions, particularly `convert_df_headers_to_nested_dict`, are very complex and hard to reason about. The logic to decide between creating a nested dictionary and a list of keys is opaque. The file also uses broad `except Exception` clauses, which can dangerously hide underlying bugs.
    *   **Recommendation**: The recursive logic should be simplified, broken down, and heavily documented with examples. Exception handling should be specific, catching only expected errors (`KeyError`, `IndexError`) and logging unexpected ones instead of silently continuing.

*   **[Inconsistent Error Handling]**: The error handling strategy in `convert_df_rows_to_nested_dict` is inconsistent. It explicitly catches some errors while allowing others to propagate based on a comment ("let the error report itself"). This makes the function's behavior unpredictable. It also makes assumptions about the input data shape (taking the first column of a DataFrame), which can hide upstream issues.
    *   **Recommendation**: Establish a consistent error handling policy. Either handle exceptions gracefully with logging or let them propagate to the caller. The function's API contract should be clarified to be more explicit about its input requirements to avoid making risky assumptions.

## `alias_handler.py`

### Purpose

This module provides a query pre-processing step called "alias enrichment." It is designed to bridge the gap between user-specific terminology and the literal data in the Excel files. By maintaining an "alias dictionary" in an external Excel file, the system can use an LLM to rewrite a user's query, replacing jargon, abbreviations, or synonyms with the correct terms before the main query processing begins.

### Key Functionality

1.  **Alias Dictionary Loading**: `get_alias_dictionary()` reads a multi-sheet Excel file and converts its contents into a single, formatted string to be used as context in an LLM prompt.
2.  **Query Enrichment**: The `AliasEnricher` class is the main orchestrator. Its `enrich_query()` method takes the user's query, combines it with the alias dictionary in a prompt, and calls the LLM to get a clarified, "enriched" version of the query.
3.  **Response Parsing**: `parse_enriched_query()` is a utility function that robustly extracts the clean, enriched query from the LLM's full text response, looking for specific headers or patterns.
4.  **Caching**: The `AliasEnricher` implements an in-memory cache to store alias dictionaries that have been read from disk, avoiding redundant file I/O for subsequent requests using the same alias file.

### Analysis and Recommendations

*   **[Security-High] Path Traversal Vulnerability**: The functions that load the alias dictionary accept a file path without proper validation. If an attacker can influence this path, they could potentially use it to read arbitrary files from the server's filesystem by crafting a malicious path string (e.g., `../../etc/passwd`).
    *   **Recommendation**: All file paths must be sanitized. Before use, paths should be resolved to their absolute form and then strictly validated to ensure they are located within a designated, secure directory for alias files.

*   **[Performance/Cost] Inefficient Token Usage**: The current implementation sends the *entire* alias dictionary to the LLM with *every single query*. For a large alias file, this is extremely inefficient, significantly increasing token consumption, API costs, and query latency.
    *   **Recommendation**: Implement a more targeted approach. Instead of sending the entire dictionary, use a retrieval mechanism. First, identify potential aliases within the user's query, then retrieve only the relevant entries from the dictionary to include in the prompt. For larger-scale applications, this could be implemented as a proper Retrieval-Augmented Generation (RAG) pipeline with vector embeddings.

*   **[Bug-Medium] Silent Failure on Error**: If the `enrich_query` method fails for any reason (e.g., LLM error, parsing failure), it silently falls back to returning the original, unenriched query. The downstream system and the user have no indication that the intended enrichment failed, which can lead to confusing results or incorrect data retrieval.
    *   **Recommendation**: Failures should be communicated explicitly. The function should either raise a custom exception or return a tuple `(enriched_query, success_flag)` to allow the calling code to handle the error gracefully, for example, by notifying the user that their query might be misinterpreted.

*   **[Maintainability] Global State**: The module relies on a global `default_alias_enricher` instance. This is risky in a multi-threaded environment (like a production web server), as it can lead to race conditions and unpredictable behavior with the shared cache and LLM instance.
    *   **Recommendation**: Avoid global state. The `AliasEnricher` instance should be managed by the application's lifecycle, created once at startup, and passed explicitly via dependency injection where needed. This improves testability and robustness.

## `prompt.py`

### Purpose

This file is the brain of the AI system. It contains the master prompts that instruct the LLM agents on how to behave. The prompts are meticulously engineered to break down the complex problem of querying hierarchical Excel data into a sequence of smaller, manageable tasks. The quality of these prompts directly determines the accuracy and reliability of the chatbot.

The overall strategy is **few-shot prompting**. Each prompt provides:
1.  A clear **role** for the AI to adopt (e.g., "You are an expert query decomposer").
2.  A detailed **problem description**, inputs, and the required output format.
3.  Several **examples** showing the exact, step-by-step reasoning (`Thinking`) and final output the model should produce.

### `DECOMPOSER_PROMPT`

*   **Agent Role**: The first agent in the query pipeline. Its job is to read the user's query and classify the key terms as either `Row Keywords` or `Col Keywords` by comparing them against the provided data hierarchies.
*   **Critique**:
    *   **[Maintainability] Hardcoded Language**: The few-shot examples are written in Vietnamese. This is a significant issue for maintainability and debugging for a non-Vietnamese-speaking team. It also limits the prompt's applicability if the bot needs to support other languages.
    *   **[Robustness] Inference-Based**: The prompt relies on the LLM's ability to infer connections (e.g., "tháng hè" meaning "hè"). While powerful, this can be brittle. A slightly different phrasing from a user could be missed.
    *   **Recommendation**: The prompts should be translated to English for easier maintenance. To make the system multilingual, a separate translation step or language-specific prompt files should be used. Adding "negative" examples (keywords that don't match anything) could improve the model's accuracy.

### `ROW_HANDLER_PROMPT`

*   **Agent Role**: The second agent. It takes the `Row Keywords` from the Decomposer and uses them to identify the precise hierarchical path(s) to the data the user wants.
*   **Key Feature**: This prompt masterfully teaches the model to use the placeholder `"Undefined"`. This is a critical instruction that allows the system to handle queries where the user skips a level in the hierarchy (e.g., asking for a "City" without specifying the "State"). It enables the construction of a complete filtering path even with incomplete input.
*   **Critique**:
    *   **[Maintainability] Hardcoded Language**: The examples are in Vietnamese.
    *   **[Complexity] Complex Logic**: The rules for using `"Undefined"` are complex and rely entirely on the LLM's ability to reason correctly. An error in applying this logic could lead to incorrect data retrieval.
    *   **Recommendation**: The explanation for `"Undefined"` is good, but the prompt could be made more robust by adding another example that illustrates its use in a different scenario, reinforcing the concept for the model.

### `COL_HANDLER_PROMPT`

*   **Agent Role**: The counterpart to the Row Handler. It takes the `Col Keywords` and identifies the precise columns the user is requesting.
*   **Key Feature**: It correctly instructs the model to *stop* at the level of specificity implied by the query. For example, if a user asks for "Time", and "Time" has sub-columns "Summer" and "Winter", the identifier correctly stops at "Time", implying all sub-columns are desired.
*   **Critique**: Similar to the other prompts, the examples are in Vietnamese, and it relies on the LLM's sometimes-unpredictable inference capabilities.

### Other Prompts

*   **`FEATURE_ANALYSIS_PROMPT`**: Used during initial file ingestion. It analyzes the file's headers to determine if it's a simple flat table or a complex matrix table, and to identify the names of the row and column hierarchies ("Feature Rows" and "Feature Cols"). This is a clever use of an LLM for automated structural analysis.
*   **`SINGLE_AGENT_PROMPT`**: A deprecated, monolithic prompt that attempts to do the job of the Decomposer, Row Handler, and Column Handler all in one step. It is notable because it **requires JSON output**, which is a much more robust approach than the custom text format used by the active agent prompts.
*   **`FILE_SUMMARY_PROMPT`**: A crucial prompt that creates a high-level, natural language "fingerprint" of a file based on its structural metadata. This semantic summary is then used by the router agent. This is an excellent design pattern.
*   **`QUERY_SEPARATOR_PROMPT`**: The router agent. It reads a user's query and all the file summaries to decide which file(s) are needed to answer the question. It can decompose a single complex query into multiple sub-queries, each targeted at the correct file.
*   **`ALIAS_HANDLE_PROMPT`**: A well-designed prompt that enriches a user's query with definitions from a dictionary. It cleverly **augments but does not replace** aliases (e.g., `CP` becomes `CP(Chi phí)`), which preserves the original term for data filtering while adding context for the LLM.

### Overall `prompt.py` Recommendations

*   **[CRITICAL] Standardize to JSON Output**: The single biggest weakness is the reliance on parsing custom text formats from the LLM. This is extremely fragile. The approach from the deprecated `SINGLE_AGENT_PROMPT` should be adopted everywhere: **all prompts that expect structured data back should be modified to require the LLM to output valid JSON**. This would eliminate the need for all the brittle custom parsers in the application and make the entire system significantly more reliable.
*   **[MAINTAINABILITY] Translate Examples**: All Vietnamese examples should be translated into English. This is a high-priority task to make the core logic of the application maintainable and understandable for a broader audience.
*   **[CLARITY] Remove Dead/Redundant Prompts**: The file should be cleaned up to only include prompts that are actively used in the production pipeline. Deprecated (`SINGLE_AGENT_PROMPT`) and potentially redundant (`SCHEMA_ANALYSIS_PROMPT`) prompts should be removed to reduce confusion.

## `llm.py`

### Purpose

This module serves as the direct interface to the language model. It is responsible for taking the formatted prompts from `prompt.py`, injecting them with runtime data (like query text and schema information), making the actual calls to the LLM, and parsing the model's text responses back into a structured format that the rest of the application can use.

### Key Functionality

1.  **Agent Chain Execution**: The `splitter()` function is the engine of the query process. It implements the multi-agent chain by:
    *   Calling the "Decomposer" agent to get row/column keywords.
    *   Calling the "Row Handler" and "Column Handler" agents with the results of the first step.
    *   Parsing the final outputs and returning the completed row and column selections.
2.  **Schema Analysis**: The `get_feature_names_from_headers()` function is used during the initial file processing. It sends the file's header content to the LLM to automatically determine the file's structure (matrix vs. flat) and identify the core feature columns.
3.  **Response Parsing**: The module contains a set of parser functions (`parse_decomposer_output`, `parse_row_handler_output`, etc.), each responsible for extracting the relevant information from the LLM's raw text output for one of the agents.

### Analysis and Recommendations

*   **[CRITICAL] Brittle and Unreliable Parsing**: The parser functions are the Achilles' heel of this architecture. They rely on custom logic to parse non-standard text formats (e.g., looking for lines starting with `-`, splitting by `###`), which is extremely fragile. A minor, unannounced formatting change from the LLM provider would cause these parsers to fail, breaking the entire query pipeline.
    *   **Recommendation**: This reinforces the recommendation for `prompt.py`: modify the prompts to require **JSON output**. If the LLM returns JSON, all of these custom parser functions can be deleted and replaced with a single, reliable call to `json.loads()`. This is the most important architectural improvement needed for the system's robustness.

*   **[MAINTAINABILITY] Redundant and Inefficient Functions**: The module contains three similar functions for analyzing a file's schema (`get_schema`, `get_feature_names`, `get_feature_names_from_headers`). One of these, `get_feature_names`, is highly inefficient as it reads the entire file into memory for a task that only requires the headers.
    *   **Recommendation**: Consolidate this logic into a single, efficient function (`get_feature_names_from_headers` is the best starting point). The redundant and inefficient functions should be removed to prevent confusion and performance issues.

*   **[MAINTAINABILITY] Use Logging, Not Printing**: The core `splitter` function is filled with `print()` statements for debugging. While useful during development, these are inappropriate for production code. They cannot be configured, filtered, or easily sent to a monitoring service.
    *   **Recommendation**: Replace all `print()` statements with structured logging calls (e.g., `logger.debug()`). This allows for configurable log levels and makes the agent's decision-making process observable in a production environment.

*   **[ROBUSTNESS] Inadequate Error Handling**: The error handling in the LLM-calling functions is not robust. It uses broad `try...except Exception` blocks that just print the error and either continue or re-raise the generic exception.
    *   **Recommendation**: Implement more robust error handling. Log errors with full tracebacks using `logger.error("...", exc_info=True)`. Wrap LLM exceptions in custom exceptions (e.g., `LLMProcessingError`) so that the calling code in `processor.py` can handle these specific failures gracefully.

## `processor.py`

### Purpose

This module is the conductor of the entire `core` orchestra. The `MultiFileProcessor` class is the main entry point and state manager for all backend processing. It initializes and calls all the other modules in the correct sequence to execute the two main pipelines: processing a new file and answering a query.

### Key Functionality

1.  **State Management**: An instance of `MultiFileProcessor` represents a single user session. It holds the LLM instance and maintains a dictionary of `FileMetadata` objects, one for every file the user has uploaded.
2.  **File Processing Pipeline (`extract_file_metadata`)**: This is the detailed, step-by-step process for ingesting an Excel file. It perfectly mirrors the process described in `FLOW.md`, calling functions from `metadata.py`, `preprocess.py`, `llm.py`, and `utils.py` to read the file, understand its structure, generate metadata, and create a semantic summary.
3.  **Query Processing Pipeline (`process_multi_file_query`)**: This method handles an incoming user query. It first uses the `AliasEnricher` to clarify the query, then calls the "Query Separator" agent to route the query to the correct file(s). It then processes the sub-queries for each file and aggregates the results.
4.  **Orchestration**: It initializes and coordinates all the different components, making sure data flows correctly from one step to the next.

### Analysis and Recommendations

*   **[Bug-Critical] Calls a Broken Function**: The `extract_file_metadata` method calls `fill_undefined_sequentially` from `preprocess.py`. As noted in the analysis of that file, the function is broken due to an indexing error and **will cause the entire file processing pipeline to crash**. This is a show-stopping bug.
    *   **Recommendation**: The underlying bug in `preprocess.py` must be fixed for the application to function at all.

*   **[ROBUSTNESS] Fragile Response Parsing**: The `parse_separator_response` function is another custom text parser designed to handle the output of the query routing agent. Like the other parsers, it is fragile and liable to break with minor LLM format changes.
    *   **Recommendation**: The `QUERY_SEPARATOR_PROMPT` should be updated to require a structured JSON output. This would allow the fragile custom parser to be replaced with a single, reliable call to `json.loads()`.

*   **[ROBUSTNESS] Silent Error Handling**: The `process_multi_file_query` method wraps the alias enrichment step in a `try...except` block that silently continues if it fails. The user is never notified that their query might be misinterpreted because an alias lookup failed.
    *   **Recommendation**: Failures in critical sub-processes like alias enrichment should be handled more explicitly, either by raising an exception or by returning a status that can be used to notify the user.

*   **[CLARITY] Ambiguous Initialization**: The `initialize_llm` function has a side-effect of setting the LLM on the `alias_enricher` instance. This makes the code less clear, as the function's actions are not fully described by its name.
    *   **Recommendation**: The `llm` instance should be passed explicitly to the `AliasEnricher` in its constructor to make the dependency clear.

## `postprocess.py`

### Purpose

This module is responsible for the final step in the data retrieval pipeline: transforming the filtered pandas DataFrame into a JSON format that is ready for the frontend to render. The `TablePostProcessor` class is specifically designed to handle complex, hierarchical tables and calculate the necessary metadata (`rowspan`, `colspan`) for them to be displayed correctly, similar to a merged-cell layout in Excel.

### Key Functionality

1.  **Hierarchical Transformation**: The core function is `build_header_matrix()`, which converts a pandas `MultiIndex` into a 2D array representing the header structure. Each element in the matrix contains the text, `colspan`, and `rowspan` attributes needed for correct HTML rendering.
2.  **Smart Spanning Logic**: It contains dedicated helper functions (`calculate_rowspan`, `calculate_intelligent_colspan`) to correctly determine how many rows or columns a given header cell should span, based on the structure of the `MultiIndex`.
3.  **Dual Output Format**: The processor generates two table formats:
    *   `normal_table`: A full hierarchical representation for rich table rendering.
    *   `flattened_table`: A simplified, single-header view of the same data. This is an excellent feature for user experience, providing a simpler alternative view.
4.  **Header Flattening**: To create the `flattened_table`, it uses a clever `create_flattened_headers` function that concatenates the hierarchical levels into a single name, using acronyms for parent levels to keep the names concise and readable.
5.  **Safe Data Serialization**: It correctly converts the DataFrame's data into JSON-safe types, ensuring that `NaN` values become `null` and numeric types are preserved.

### Analysis and Recommendations

*   **[ROBUSTNESS] Tight Coupling**: The `calculate_rowspan` logic is tightly coupled to the preprocessing step, as it specifically expects placeholder cells to be named `"Header"`. If this convention in `preprocess.py` were to change, this module would calculate row spans incorrectly.
    *   **Recommendation**: This is an acceptable risk in a controlled system, but it should be documented in both modules that this dependency exists.

*   **[COMPLEXITY] Inherently Complex Logic**: The task of calculating row and column spans for an arbitrary hierarchy is inherently complex. The code that accomplishes this is well-structured but can be difficult to debug due to the nested loops and state tracking (`covered_positions`). This is more of an observation of the problem's difficulty than a critique of the implementation.

*   **[CLARITY] Acronym Logic**: The `create_acronym` function, while clever, has somewhat convoluted logic for handling strings with mixed letters and numbers. This could lead to unpredictable acronyms for certain edge cases.
    *   **Recommendation**: The logic could be simplified or replaced with a more straightforward and predictable regex pattern to improve maintainability.

*   **Overall**: This is a strong, well-engineered module. The detailed logging is particularly good and serves as a model for other parts of the system. It successfully solves a very challenging and important part of the application.

## `utils.py`

### Purpose

This module is a toolbox of helper functions used by the other `core` modules. It centralizes common, reusable logic for tasks like data manipulation and formatting.

### Key Functionality

1.  **Fuzzy Column Matching (`get_feature_name_content`)**: This is a critical utility. When the LLM provides an *expected* column name, this function intelligently maps it to the *actual* column name in the DataFrame. It uses fuzzy string matching to account for minor differences in spelling, spacing, or capitalization, making the system more resilient.
2.  **Hierarchical Formatting (`format_row_dict_for_llm`, `format_col_dict_for_llm`)**: These functions are the "encoding" side of the LLM communication pipeline. They take the nested dictionary structures generated by the `metadata` module and convert them into the specific indentation-based text formats that the prompts in `prompt.py` expect.
3.  **File Reading (`read_file`)**: A simple wrapper for reading an Excel file and converting it to a CSV string for processing.

### Analysis and Recommendations

*   **[Bug-High] Flawed MultiIndex Matching**: The `get_feature_name_content` function contains a significant bug in how it handles MultiIndex columns. When it finds a match on a top-level column name (e.g., "Sales"), it always selects the *first* corresponding column tuple (e.g., `('Sales', 'Q1')`) and ignores all others (e.g., `('Sales', 'Q2')`, `('Sales', 'Q3')`). This will lead to entire sections of a file being ignored by the system.
    *   **Recommendation**: This logic needs to be corrected to ensure that all columns under a matched top-level header are considered and processed, not just the first one encountered.

*   **[CRITICAL] Application Termination**: The `read_file` function contains an `exit(1)` call within its exception handler. A utility function within a library must **never** terminate the entire application. This is a critical bug.
    *   **Recommendation**: Replace `exit(1)` with a `raise` statement to propagate the exception to the calling code, which can then handle it gracefully.

*   **[ROBUSTNESS] Creates Brittle Format**: The `format_*_dict_for_llm` functions exist solely to create the fragile, non-standard text formats required by the current prompts. They are a necessary evil in the current architecture but are part of the core problem.
    *   **Recommendation**: These functions are a primary reason to switch the prompt strategy to require JSON output. If that change is made, these complex formatters can be completely deleted and replaced with `json.dumps()`.

*   **[MAINTAINABILITY] Use Logging, Not Printing**: The module uses `print()` to report fuzzy matching mismatches. This should be converted to proper logging (`logger.warning`) to be consistent and configurable.