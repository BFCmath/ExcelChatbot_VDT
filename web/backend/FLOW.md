# System Flow Documentation

This document outlines the data and logic flow for the Excel Chatbot system. The system has two main operational modes: a `Single-File Mode` for interacting with one Excel file, and a `Multi-File Mode` for handling queries that may involve several Excel files.

---

## Core Components

The system is built from several key Python scripts, each with a specific role:

-   **`llm.py`**: Manages all interactions with the Large Language Model (LLM). It contains functions to call the LLM with different prompts and parse the responses. It defines the logic for different "agents" (e.g., `splitter`, `single_agent`).
-   **`prompt.py`**: A central repository for all prompt templates used to instruct the LLM. This includes prompts for analyzing file structure, summarizing files, decomposing queries, and identifying row/column selections.
-   **`metadata.py`**: Contains functions for extracting structural metadata from Excel files. This includes determining the number of header rows and converting hierarchical row and column structures into nested dictionaries.
-   **`preprocess.py`**: Provides utility functions for cleaning and preparing the data within pandas DataFrames, such as filling missing values and cleaning unnamed headers.
-   **`extract_df.py`**: Responsible for taking the structured row and column selections generated by the LLM and applying them as filters to the pandas DataFrame to extract the final data.
-   **`utils.py`**: A collection of general utility functions, including reading Excel files and formatting the extracted metadata into a string format suitable for LLM prompts.
-   **`reporter.py`**: Manages the logging of queries, LLM responses, and final results into reports for debugging and review.
-   **`singlefile.py`**: The entry point and controller for the Single-File Mode.
-   **`multifile.py`**: The entry point and controller for the Multi-File Mode, which includes advanced features like caching and query routing.

---

## Single-File Mode Flow (`singlefile.py`)

This mode provides an interactive command-line interface (CLI) to "chat" with a single Excel file.

**1. Initialization & Preprocessing:**
    a. The script starts by loading a specific Excel file into a pandas DataFrame.
    b. **Feature Identification**: It calls `llm.get_feature_names` which uses the `FEATURE_ANALYSIS_PROMPT` to ask the LLM to identify the "feature rows" (columns defining the row hierarchy) and "feature columns" (columns defining the column hierarchy).
    c. **Metadata Extraction**:
        - It uses `metadata.get_number_of_row_header` to determine the header size.
        - The DataFrame is re-read using the correct header information.
        - The DataFrame is cleaned using functions from `preprocess.py`.
    d. **Structure Creation**:
        - `metadata.convert_df_rows_to_nested_dict` and `metadata.convert_df_headers_to_nested_dict` are used to build nested dictionary representations of the row and column hierarchies.
        - `utils.format_row_dict_for_llm` and `utils.format_col_dict_for_llm` convert these dictionaries into a readable string format for the LLM.

**2. Interactive Query Loop:**
    a. The user enters a natural language query.
    b. The user can choose between two agent modes: `single` or `splitter`.
    c. **`splitter` Mode (Default & Advanced):**
        i. **Decomposition**: `llm.splitter` is called. This first uses the `DECOMPOSER_PROMPT` to break the user's query into "row keywords" and "column keywords".
        ii. **Row Handling**: The "row keywords" are passed to the `ROW_HANDLER_PROMPT` to get a structured `row_selection`.
        iii. **Column Handling**: The "column keywords" are passed to the `COL_HANDLER_PROMPT` to get a structured `col_selection`.
        iv. **Data Extraction**: The `row_selection` and `col_selection` are passed to `extract_df.render_filtered_dataframe`. This function parses the selections and applies them as filters to the DataFrame.
        v. **Display**: The resulting filtered DataFrame is displayed to the user.
    d. **`single` Mode (Legacy):**
        i. `llm.single_agent` is called with the `SINGLE_AGENT_PROMPT`. This single, complex prompt asks the LLM to perform decomposition, row selection, and column selection in one step.
        ii. The raw JSON output from the LLM is displayed to the user, without a final data extraction step.
    e. **Reporting**: In both modes, the query, LLM output, and (in splitter mode) the final DataFrame are saved to a report via `reporter.py`.

---

## Multi-File Mode Flow (`multifile.py`)

This mode is designed to handle queries that may span multiple Excel files. It introduces caching, file summarization, and intelligent query routing.

**1. Initialization & Metadata Caching:**
    a. The `MultiFileProcessor` is initialized. On startup, it loads a cache of file metadata from `cache/file_metadata.pkl` if it exists.
    b. The script processes a directory of Excel files. For each file:
        i. It calculates the file's MD5 hash and checks if a valid, up-to-date cache for this file exists.
        ii. **If not cached or changed**: It runs the full metadata extraction pipeline (same as steps 1b-1d in the single-file flow).
        iii. **File Summarization**: After extracting metadata, it calls `generate_file_summary`. This uses the `FILE_SUMMARY_PROMPT` to ask the LLM to create a concise, natural language summary of the file's structure and content.
        iv. The new metadata and summary are saved to the `file_metadata` object and persisted in the cache file.

**2. Interactive Query Loop:**
    a. The user enters a natural language query.
    b. **Query Routing (Separation)**:
        i. The core of the multi-file mode. The user's query along with the pre-computed summaries of all available files are passed to the LLM using the `QUERY_SEPARATOR_PROMPT`.
        ii. The LLM acts as a "router", analyzing the query and the summaries to determine which file(s) are best suited to answer the query.
        iii. The LLM returns a list of "assignments", breaking the original query down into one or more sub-queries, each targeted at a specific file.
    c. **Sub-query Execution:**
        i. The system iterates through each assignment from the separator.
        ii. For each `(filename, sub_query)` pair, it retrieves the cached metadata for that file.
        iii. It executes the `splitter` agent logic (as described in the single-file flow, steps 2.c.i - 2.c.iv) on that file's DataFrame using the `sub_query`.
    d. **Result Aggregation & Display**:
        i. The filtered DataFrames resulting from each sub-query are collected.
        ii. All resulting tables are displayed to the user.
    e. **Reporting**: All steps, including the separation and the results of each sub-query, are logged via `reporter.py`. 